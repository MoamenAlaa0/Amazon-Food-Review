{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\K I N\n",
      "[nltk_data]     G\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\K I N\n",
      "[nltk_data]     G\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "# Text preprocessing packages\n",
    "import nltk # Text libarary\n",
    "# nltk.download('stopwords')\n",
    "import string # Removing special characters {#, @, ...}\n",
    "import re  # Regex Package\n",
    "from nltk.corpus import stopwords # Stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer # Stemmer & Lemmatizer\n",
    "from gensim.utils import simple_preprocess  # Text ==> List of Tokens\n",
    "\n",
    "# Text Embedding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Saving Model\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Visualization Packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=1.3)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'568,454 Review'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{df.shape[0]:,} Review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score\n",
       "0  I have bought several of the Vitality canned d...      5\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      1\n",
       "2  This is a confection that has been around a fe...      4\n",
       "3  If you are looking for the secret ingredient i...      2\n",
       "4  Great taffy at a great price.  There was a wid...      5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Text', 'Score']\n",
    "df_text = df[cols].copy()\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates\n",
    "Save the Cleaned data-frame also with the variable `df_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.drop_duplicates(subset = ['Text'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393579"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Pre-Processing\n",
    "`target` will be \n",
    " - 0 if score < 3 \n",
    " - 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text['target'] = [0 if x < 3 else 1 for x in df_text['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score  target\n",
       "0  I have bought several of the Vitality canned d...      5       1\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      1       0\n",
       "2  This is a confection that has been around a fe...      4       1\n",
       "3  If you are looking for the secret ingredient i...      2       0\n",
       "4  Great taffy at a great price.  There was a wid...      5       1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Countplot for target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'target', data = df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how such variance is huge ...   \n",
    "Then we need to down-sample such data ... by which both the positive and negative classes are balanced.\n",
    "\n",
    "### Balance Data Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.reset_index(drop = True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from positive reviews Same number of negative reviews\n",
    "NEG_N = df_text.target.value_counts()[0]\n",
    "df_pos = df_text[df_text['target'] == 1]['Text'].sample(NEG_N, replace=False)\n",
    "df_text_balanced = pd.concat([df_text.iloc[df_pos.index], df_text[df_text.target == 0]]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103517</td>\n",
       "      <td>I was skeptical at first -- could we commit to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>374703</td>\n",
       "      <td>I have been eating Kameda Kaki-no-tane (its ja...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180188</td>\n",
       "      <td>This is the best price I've found on Kuerig cu...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>210266</td>\n",
       "      <td>This is the best tasting gluten free pizza dou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160255</td>\n",
       "      <td>This is a perfect Chai Latte!  No need to add ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                               Text  Score  target\n",
       "0  103517  I was skeptical at first -- could we commit to...      5       1\n",
       "1  374703  I have been eating Kameda Kaki-no-tane (its ja...      5       1\n",
       "2  180188  This is the best price I've found on Kuerig cu...      5       1\n",
       "3  210266  This is the best tasting gluten free pizza dou...      5       1\n",
       "4  160255  This is a perfect Chai Latte!  No need to add ...      5       1"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_balanced.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLot the target again after balancing\n",
    "sns.countplot(x = 'target', data = df_text_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text      0\n",
       "Score     0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_balanced.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove(text):\n",
    "    text = re.sub(\"\\\\<.*?\\\\>\", \"\", text)  # Removing tags html from string\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)  # Removing links from string\n",
    "    text = re.sub(\"[^A-Za-z']+\", \" \", text)   # Removing any brackets or special character or numbers \n",
    "    text = re.sub(r'(Mr|Ms|Mrs)\\.?\\s[A-Z]\\w*', \" \", text) # Removing names from strings \n",
    "    text = \" \".join(text.split()) # adjusting the spaces in string\n",
    "    return text\n",
    "\n",
    "df_text_balanced['Text']= df_text_balanced['Text'].str.lower().str.strip().apply(Remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stops = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('not')\n",
    "stop_words.update(list_of_stops)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWords(text):\n",
    "    x = ' '\n",
    "    ls = list()\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            ls.append(word)\n",
    "    return x.join(ls)\n",
    "    \n",
    "df_text_balanced['Text'] = df_text_balanced['Text'].apply(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skeptical commit bags coffee organic mocha jav...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eating kameda kaki tane japanese decades versi...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price kuerig cups arrives time earlier expecte...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tasting gluten free pizza dough simple find go...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect chai latte add cream sugar perfect coo...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score  target\n",
       "0  skeptical commit bags coffee organic mocha jav...      5       1\n",
       "1  eating kameda kaki tane japanese decades versi...      5       1\n",
       "2  price kuerig cups arrives time earlier expecte...      5       1\n",
       "3  tasting gluten free pizza dough simple find go...      5       1\n",
       "4  perfect chai latte add cream sugar perfect coo...      5       1"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    x = ' '\n",
    "    ls = list()\n",
    "    for word in text.split():\n",
    "        ls.append(lemmatizer.lemmatize(word))\n",
    "    return x.join(ls)\n",
    "    \n",
    "df_text_balanced['Text'] = df_text_balanced['Text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skeptical commit bag coffee organic mocha java...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eating kameda kaki tane japanese decade versio...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price kuerig cup arrives time earlier expected...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tasting gluten free pizza dough simple find go...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect chai latte add cream sugar perfect coo...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score  target\n",
       "0  skeptical commit bag coffee organic mocha java...      5       1\n",
       "1  eating kameda kaki tane japanese decade versio...      5       1\n",
       "2  price kuerig cup arrives time earlier expected...      5       1\n",
       "3  tasting gluten free pizza dough simple find go...      5       1\n",
       "4  perfect chai latte add cream sugar perfect coo...      5       1"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    x = ' '\n",
    "    ls = list()\n",
    "    for word in text.split():\n",
    "        ls.append(stemmer.stem(word))\n",
    "    return x.join(ls)\n",
    "\n",
    "df_text_balanced['Text'] = df_text_balanced['Text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skeptic commit bag coffe organ mocha java tast...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eat kameda kaki tane japanes decad version mee...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price kuerig cup arriv time earlier expect cof...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tast gluten free pizza dough simpl find good g...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect chai latt add cream sugar perfect cool...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score  target\n",
       "0  skeptic commit bag coffe organ mocha java tast...      5       1\n",
       "1  eat kameda kaki tane japanes decad version mee...      5       1\n",
       "2  price kuerig cup arriv time earlier expect cof...      5       1\n",
       "3  tast gluten free pizza dough simpl find good g...      5       1\n",
       "4  perfect chai latt add cream sugar perfect cool...      5       1"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Test & Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_text_balanced['Text']\n",
    "y = df_text_balanced['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding\n",
    " - Use `TfidfVectorizer`\n",
    " - `fit` on the training data only\n",
    " - `transform` on training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFIDF embedding for the Description\n",
    "vectorizer = TfidfVectorizer()\n",
    "# fit on training (such vectorizer will be saved for deployment)\n",
    "vectorizer_tfidf = vectorizer.fit(X)\n",
    "# transform on training data\n",
    "X_train = vectorizer_tfidf.transform(X_train)\n",
    "# transform on testing data\n",
    "X_test = vectorizer_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79893, 47553), (34241, 47553))"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the dimensions of your data embeddings before entering to the model\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sklearn framework steps\n",
    " - init\n",
    " - fit\n",
    " - predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 82.75%\n"
     ]
    }
   ],
   "source": [
    "## initialize your Model\n",
    "clf = RandomForestClassifier(random_state=1) \n",
    "# Fit your Model on the Training Dataset\n",
    "clf.fit(X_train, y_train)\n",
    "# Predict on Test data\n",
    "preds = clf.predict(X_test)\n",
    "# Calculate Model Accuracy\n",
    "acc = accuracy_score(preds, y_test)\n",
    "print(f\"Model Accuracy = {round(acc*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Instance Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_test(review, model, vectorizer):\n",
    "    # Clean Review\n",
    "    review_c = stemming(lemmatization(stopWords(Remove(review.lower().strip()))))\n",
    "    # Embed review using tf-idf vectorizer\n",
    "    embedding = vectorizer.transform([review_c])\n",
    "    # Predict using your model\n",
    "    prediction = model.predict(embedding)\n",
    "    # Return the Sentiment Prediction\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = \"That's a good Dish, Good Job\"\n",
    "review_2 = \"That's the worst Dish ever tasted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test(review_1, clf, vectorizer_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test(review_2, clf, vectorizer_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rf_model.pk'\n",
    "vectorizer_name = 'tfidf_vectorizer.pk'\n",
    "model_path = os.path.join('C:/Users/K I N G/Nawah Projects/Food Review/pickling/', model_name)\n",
    "vect_path = os.path.join('C:/Users/K I N G/Nawah Projects/Food Review/pickling/', vectorizer_name)\n",
    "\n",
    "pickle.dump(clf, open(model_path, 'wb')) ## Save model\n",
    "pickle.dump(vectorizer_tfidf, open(vect_path,'wb')) ## Save tfidf-vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model Again and test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(model_path, 'rb'))\n",
    "loaded_vect = pickle.load(open(vect_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test(review_1, loaded_model, loaded_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test(review_2, loaded_model, loaded_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NLP_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NLP_analysis.py\n",
    "\n",
    "# cd 'C:\\Users\\K I N G\\Nawah Projects\\Food Review'\n",
    "\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import nltk # Text libarary\n",
    "import string # Removing special characters {#, @, ...}\n",
    "import re  # Regex Package\n",
    "from nltk.corpus import stopwords # Stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer # Stemmer & Lemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "model_path = 'C:/Users/K I N G/Nawah Projects/Food Review/pickling/rf_model.pk'\n",
    "vect_path = 'C:/Users/K I N G/Nawah Projects/Food Review/pickling/tfidf_vectorizer.pk'\n",
    "\n",
    "loaded_model = pickle.load(open(model_path, 'rb'))\n",
    "loaded_vect = pickle.load(open(vect_path, 'rb'))\n",
    "\n",
    "def Remove(text):\n",
    "    text = re.sub(\"\\\\<.*?\\\\>\", \"\", text)  # Removing tags html from string\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)  # Removing links from string\n",
    "    text = re.sub(\"[^A-Za-z']+\", \" \", text)   # Removing any brackets or special character or numbers \n",
    "    text = re.sub(r'(Mr|Ms|Mrs)\\.?\\s[A-Z]\\w*', \" \", text) # Removing names from strings \n",
    "    text = \" \".join(text.split()) # adjusting the spaces in string\n",
    "    return text\n",
    "\n",
    "list_of_stops = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('not')\n",
    "stop_words.update(list_of_stops)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "def stopWords(text):\n",
    "    x = ' '\n",
    "    ls = list()\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            ls.append(word)\n",
    "    return x.join(ls)\n",
    "\n",
    "def lemmatization(text):\n",
    "    x = ' '\n",
    "    ls = list()\n",
    "    for word in text.split():\n",
    "        ls.append(lemmatizer.lemmatize(word))\n",
    "    return x.join(ls)\n",
    "\n",
    "def stemming(text):\n",
    "    x = ' '\n",
    "    ls = list()\n",
    "    for word in text.split():\n",
    "        ls.append(stemmer.stem(word))\n",
    "    return x.join(ls)\n",
    "\n",
    "def raw_test(review, model, vectorizer):\n",
    "    review_c = stemming(lemmatization(stopWords(Remove(review.lower().strip()))))\n",
    "    embedding = vectorizer.transform([review_c])\n",
    "    prediction = model.predict(embedding)\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\" \n",
    "\n",
    "def main():\n",
    "    st.title('Amazon Food Review')\n",
    "    st.subheader(\"Analyze The Text\")\n",
    "    text = st.text_area('')\n",
    "    predict = raw_test(text, loaded_model, loaded_vect)\n",
    "    if st.button('Analyze'):\n",
    "            st.success(predict)\n",
    "        else: st.warning('Negative')\n",
    "\n",
    "if __name__=='__main__': \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run NLP_analysis.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
